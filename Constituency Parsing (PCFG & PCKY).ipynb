{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCFG\n",
    "\n",
    "## Motivation: ambiguity\n",
    "\n",
    "One crucial use of probabilistic parsing is to solve the problem of disambiguation.   \n",
    "  \n",
    "The CKY parsing algorithm can represent ambiguities like **coordination ambiguity** and **attachment ambiguity** in an efﬁcient way but is not equipped to resolve them.   \n",
    "  \n",
    "A general solution to those problems is adding probability, that is to compute the probability of each interpretation and choose the most probable interpretation.   \n",
    "  \n",
    "The most commonly used probabilistic constituency grammar formalism is the **probabilistic context-free grammar (PCFG)**, a probabilistic augmentation of context-free grammars in which each rule is associated with a probability.\n",
    "\n",
    "## Definition\n",
    "\n",
    "PCFGs are augmentation of context-free grammars, that is, each rule now has a corresponding probability.  \n",
    "  \n",
    "* A PCFG is consistent if sum of probabilities of all sentences in language is 1.\n",
    "\n",
    "\n",
    "## Learn the PCFG Rule Probabilities\n",
    "The simplest way is to use a treebank, a corpus of already parsed sentences. Given a treebank, we can compute the probability of each expansion of a non-terminal by counting the number of times that expansion occurs and then normalizing.\n",
    "\n",
    "$$P(\\alpha \\to \\beta \\mid \\alpha) = \\frac{Count(\\alpha \\to \\beta)}{Count(\\alpha)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import Tree\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def update_production_cnt(tree_str, c):\n",
    "    tree = Tree.fromstring(tree_str)\n",
    "    for p in tree.productions():\n",
    "        c[p.lhs()].update([p])\n",
    "    return \n",
    "\n",
    "def calculate_prob(c):\n",
    "    d = defaultdict(dict)\n",
    "    for key, cnt in c.items():\n",
    "        size = sum(cnt.values())\n",
    "        for rule, occ in cnt.items():\n",
    "            d[key][rule] = occ / size\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcfg_cnt = defaultdict(Counter)\n",
    "\n",
    "treebank_filename = 'data/treebank.txt'\n",
    "\n",
    "with open(treebank_filename, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if line.strip():\n",
    "            update_production_cnt(line.strip(), pcfg_cnt)\n",
    "    pcfg = calculate_prob(pcfg_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look of some rules and probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{TOP -> SQ PUNC: 0.048638132295719845,\n",
       " TOP -> S PUNC: 0.08949416342412451,\n",
       " TOP -> SBARQ PUNC: 0.23540856031128404,\n",
       " TOP -> S_VP PUNC: 0.2140077821011673,\n",
       " TOP -> FRAG_NP PUNC: 0.2237354085603113,\n",
       " TOP -> FRAG PUNC: 0.054474708171206226,\n",
       " TOP -> INTJ_UH PUNC: 0.0038910505836575876,\n",
       " TOP -> FRAG_NP_NN PUNC: 0.0038910505836575876,\n",
       " TOP -> SBAR PUNC: 0.0019455252918287938,\n",
       " TOP -> FRAG_VP PUNC: 0.019455252918287938,\n",
       " TOP -> NP PUNC: 0.005836575875486381,\n",
       " TOP -> FRAG_NP_NNP PUNC: 0.0019455252918287938,\n",
       " TOP -> FRAG_WHNP PUNC: 0.07392996108949416,\n",
       " TOP -> FRAG_PP PUNC: 0.011673151750972763,\n",
       " TOP -> FRAG_ADJP_JJ PUNC: 0.0038910505836575876,\n",
       " TOP -> ADJP_JJ PUNC: 0.0019455252918287938,\n",
       " TOP -> FRAG_ADJP_JJS PUNC: 0.0019455252918287938,\n",
       " TOP -> FRAG_NP_NNS PUNC: 0.0019455252918287938,\n",
       " TOP -> X_S_VP PUNC: 0.0019455252918287938}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcfg[list(pcfg.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/grammar.pcfg', 'w') as f:\n",
    "    f.write('%start TOP\\n')\n",
    "    for key, cnt in pcfg.items():\n",
    "        for rule, p in cnt.items():\n",
    "            f.write('{} [{}]\\n'.format(rule, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCKY\n",
    "PCKY produces the most-likely parse for a given sentence, it's based on CKY, and we output the most probable parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def parse(sent, grammar):\n",
    "    '''\n",
    "    sent: list of words\n",
    "    grammar: NLTK PCFG grammar\n",
    "    '''\n",
    "    table = [[None]*len(sent) for _ in range(len(sent))]\n",
    "    for j in range(len(sent)):\n",
    "        table[j][j] = [(rule.lhs(), Tree(str(rule.lhs()), rule.rhs()), rule.prob()) for rule in grammar.productions(rhs=sent[j])]\n",
    "        for i in range(j)[::-1]:\n",
    "            for k in range(i, j):\n",
    "                l1, l2 = table[i][k], table[k+1][j]\n",
    "                l3 = table[i][j] if table[i][j] else []\n",
    "                if l1 and l2:\n",
    "                    for i1 in l1:\n",
    "                        for i2 in l2:\n",
    "                            s1, tree1, prob1 = i1\n",
    "                            s2, tree2, prob2 = i2\n",
    "                            possible_rule = grammar.productions(rhs=s1)\n",
    "                            for r in possible_rule:\n",
    "                                if r.rhs() == (s1, s2):\n",
    "                                    l3 += [(r.lhs(), Tree(str(r.lhs()), [tree1, tree2]), prob1*prob2*r.prob())]\n",
    "                    table[i][j] = l3\n",
    "    return table\n",
    "\n",
    "def parse_sentence(sent, grammar):\n",
    "    table = parse(word_tokenize(sent), grammar)\n",
    "    if table[0][-1]:\n",
    "        tree = max(table[0][-1], key=lambda x: x[2])\n",
    "        if tree[0] == pcfg.start():\n",
    "            print(tree[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP\n",
      "  (S_VP\n",
      "    (S_VP_PRIME (VB Show) (NP_PRP me))\n",
      "    (NP\n",
      "      (NP_PRIME\n",
      "        (NP\n",
      "          (NP (DT all) (NNS flights))\n",
      "          (SBAR\n",
      "            (WHNP_WDT that)\n",
      "            (S_VP\n",
      "              (VBP depart)\n",
      "              (PP (IN before) (NP (CD ten) (RB a.m))))))\n",
      "        (CC and))\n",
      "      (VP (VBP have) (NP (JJ first) (NN class)))))\n",
      "  (PUNC .))\n"
     ]
    }
   ],
   "source": [
    "pcfg = nltk.data.load('data/grammar.pcfg')\n",
    "\n",
    "text = 'Show me all flights that depart before ten a.m and have first class .'\n",
    "\n",
    "parse_sentence(text, pcfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
