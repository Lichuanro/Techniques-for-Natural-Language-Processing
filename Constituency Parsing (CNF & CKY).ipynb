{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constituency Parsing\n",
    "Syntactic parsing is the task of recognizing a sentence and assigning a syntactic structure to it.  \n",
    "  \n",
    "Ambiguity is the most serious problem faced by syntactic parsers. Structural ambiguity occurs when the grammar can assign more than one parse to a sentence. Two common kinds of it are attachment ambiguity and coordination ambiguity.  \n",
    "  \n",
    "A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place.  \n",
    "  \n",
    "In coordination ambiguity different sets of phrases can be conjoined by a conjunction like and."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chomsky Normal Form\n",
    "Cocke-Kasami-Younger (CKY) algorithm is the most widely used dynamic-programming based approach to parsing, and it requires that grammars used with it must be in Chomsky Normal Form (CNF). In CNF, the right-hand side of each rule must expand either to two nonterminals or to a single terminal.\n",
    "\n",
    "## Converting CFG to CNF\n",
    "There are three situations we need to solve:\n",
    "* rules that mix terminals with non-terminals on the right-hand side\n",
    "* rules that have a single non-terminal on the right-hand side\n",
    "* rules in which the length of the right-hand side is greater than 2\n",
    "\n",
    "For mix terminals and non-terminals, we can simply introduce a new dummy non-terminal that covers only the original terminal. e.g. INF-VP → to VP can be replaced by INF-VP → TO VP and TO → to\n",
    "\n",
    "Rules with right-hand sides longer than 2 are normalized through the introduction of new non-terminals that spread the longer sequences over several new rules.\n",
    "\n",
    "Rules with a single non-terminal on the right are called unit productions. We can eliminate unit productions by rewriting the right-hand side of the original rules with the right-hand side of all the non-unit production rules that they ultimately lead to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.grammar import CFG, Nonterminal, Production\n",
    "\n",
    "cfg = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N N N | NP PP | Det Adj N \n",
    "VP -> V | V NP | VP PP\n",
    "Det -> 'a' | 'the'\n",
    "Adj -> 'quick' | 'brown' \n",
    "N -> 'dog' | 'cat' | 'mat'\n",
    "V -> 'chased' | 'sat'\n",
    "P -> 'on' | 'in'\n",
    "NP -> 'the' Nom\n",
    "\"\"\")\n",
    "\n",
    "c = 0 # count of intermediate rule X_c\n",
    "to_convert = [] # for rules to be converted except mixed cases\n",
    "mixed = [] # for mix terminals and non-terminals\n",
    "result = []\n",
    "\n",
    "for p in cfg.productions():\n",
    "    if p.is_lexical() and len(p.rhs()) > 1:\n",
    "        mixed.append(p)\n",
    "    else:\n",
    "        to_convert.append(p)\n",
    "\n",
    "for p in mixed:\n",
    "    l, r = p.lhs(), p.rhs()\n",
    "    new_r = []\n",
    "    for t in r:\n",
    "        if type(t) == str: \n",
    "            new_r.append(Nonterminal('_X{}_'.format(c)))\n",
    "            result.append(Production(Nonterminal('_X{}_'.format(c)), (t,)))\n",
    "            c += 1\n",
    "        else:\n",
    "            new_r.append(t)\n",
    "    if len(new_r) == 2:\n",
    "        result.append(Production(l, new_r))\n",
    "    else:\n",
    "        to_convert.append(Production(l, new_r))\n",
    "\n",
    "for rule in to_convert:\n",
    "    if len(rule.rhs()) > 2:\n",
    "        l = rule.lhs()\n",
    "        for k in range(0, len(rule.rhs()) - 2):\n",
    "            r = rule.rhs()[k]\n",
    "            inter = Nonterminal('_X{}_'.format(c))\n",
    "            c += 1\n",
    "            new_rule = Production(l, (r, inter))\n",
    "            l = inter\n",
    "            result.append(new_rule)\n",
    "        last = Production(l, rule.rhs()[-2:])\n",
    "        result.append(last)\n",
    "    else:\n",
    "        result.append(rule)\n",
    "cnf = CFG(cfg.start(), result)\n",
    "\n",
    "result = []\n",
    "unitary = [] # For rules with a single non-terminal\n",
    "\n",
    "for rule in cnf.productions():\n",
    "    if len(rule) == 1 and rule.is_nonlexical():\n",
    "        unitary.append(rule)\n",
    "    else:\n",
    "        result.append(rule)\n",
    "\n",
    "while unitary:\n",
    "    rule = unitary.pop(0)\n",
    "    for item in cnf.productions(lhs=rule.rhs()[0]):\n",
    "        new_rule = Production(rule.lhs(), item.rhs())\n",
    "        if len(new_rule) != 1 or new_rule.is_lexical():\n",
    "            result.append(new_rule)\n",
    "        else:\n",
    "            unitary.append(new_rule)\n",
    "\n",
    "cnf = CFG(cnf.start(), result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 25 productions (start state = S)\n",
      "    _X0_ -> 'the'\n",
      "    NP -> _X0_ Nom\n",
      "    S -> NP VP\n",
      "    PP -> P NP\n",
      "    NP -> Det _X1_\n",
      "    _X1_ -> N _X2_\n",
      "    _X2_ -> N N\n",
      "    NP -> NP PP\n",
      "    NP -> Det _X3_\n",
      "    _X3_ -> Adj N\n",
      "    VP -> V NP\n",
      "    VP -> VP PP\n",
      "    Det -> 'a'\n",
      "    Det -> 'the'\n",
      "    Adj -> 'quick'\n",
      "    Adj -> 'brown'\n",
      "    N -> 'dog'\n",
      "    N -> 'cat'\n",
      "    N -> 'mat'\n",
      "    V -> 'chased'\n",
      "    V -> 'sat'\n",
      "    P -> 'on'\n",
      "    P -> 'in'\n",
      "    VP -> 'chased'\n",
      "    VP -> 'sat'\n"
     ]
    }
   ],
   "source": [
    "print(cnf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKY Parsing\n",
    "Cocke-Kasami-Younger parsing algorithm is an efficient bottom-up parsing algorithm based on tabulating substring parses to avoid repeated work.\n",
    "\n",
    "Approach:\n",
    "* use a CNF grammar\n",
    "* build a matrix to store subtrees\n",
    "* incrementally build parse spanning whole input string\n",
    "\n",
    "The complexity of CKY Parsing is $O(n^3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def parse(sent, grammar):\n",
    "    '''\n",
    "    sent: list of words\n",
    "    grammar: NLTK grammar\n",
    "    '''\n",
    "    table = [[None]*len(sent) for _ in range(len(sent))]\n",
    "    for j in range(len(sent)):\n",
    "        table[j][j] = [(rule.lhs(), Tree(str(rule.lhs()), rule.rhs())) for rule in grammar.productions(rhs=sent[j])]\n",
    "        for i in range(j)[::-1]:\n",
    "            for k in range(i, j):\n",
    "                l1, l2 = table[i][k], table[k+1][j]\n",
    "                l3 = table[i][j] if table[i][j] else []\n",
    "                if l1 and l2:\n",
    "                    for i1 in l1:\n",
    "                        for i2 in l2:\n",
    "                            s1, tree1 = i1\n",
    "                            s2, tree2 = i2\n",
    "                            possible_rule = grammar.productions(rhs=s1)\n",
    "                            for r in possible_rule:\n",
    "                                if r.rhs() == (s1, s2):\n",
    "                                    l3 += [(r.lhs(), Tree(str(r.lhs()), [tree1, tree2]))]\n",
    "                    table[i][j] = l3\n",
    "    return table\n",
    "    \n",
    "def print_tree(table):\n",
    "    total = len(table[0][-1])\n",
    "    for tree in table[0][-1]:\n",
    "        if tree[0] == grammar.start():\n",
    "            print(tree[1])\n",
    "\n",
    "def parse_sentence(s, grammar):\n",
    "    print(s)\n",
    "    print_tree(parse(word_tokenize(s), grammar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists have discovered that malaria invades the whole body.\n",
      "(TOP\n",
      "  (NP Scientists)\n",
      "  (VP\n",
      "    (AUX have)\n",
      "    (VP\n",
      "      (VBN discovered)\n",
      "      (SBAR\n",
      "        (_X_2 that)\n",
      "        (S\n",
      "          (NP malaria)\n",
      "          (_X_5\n",
      "            (VP\n",
      "              (VBZ invades)\n",
      "              (NP (Det the) (Nom (ADJP whole) (Nom body))))\n",
      "            (PUNC .)))))))\n",
      "(TOP\n",
      "  (NP Scientists)\n",
      "  (_X_6\n",
      "    (VP\n",
      "      (AUX have)\n",
      "      (VP\n",
      "        (VBN discovered)\n",
      "        (SBAR\n",
      "          (_X_2 that)\n",
      "          (S\n",
      "            (NP malaria)\n",
      "            (VP\n",
      "              (VBZ invades)\n",
      "              (NP (Det the) (Nom (ADJP whole) (Nom body))))))))\n",
      "    (PUNC .)))\n"
     ]
    }
   ],
   "source": [
    "grammar_filename = 'data/grammar_cnf.cfg'\n",
    "grammar = nltk.data.load(grammar_filename)\n",
    "\n",
    "text = 'Scientists have discovered that malaria invades the whole body.'\n",
    "\n",
    "parse_sentence(text,grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
